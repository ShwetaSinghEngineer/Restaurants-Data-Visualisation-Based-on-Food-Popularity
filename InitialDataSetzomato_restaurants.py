# -*- coding: utf-8 -*-
"""meta data for zomato restaurants.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BsxGh_8TtRyhJLCphk0hhH2D2knAO9Tt

<h1 align=center><font size = 5><em>Meta</em> Data Scraper</font></h1>

## Introduction

This notebook contains the code to scrape meta data for each of the restaurant listed in zomato. The meta data includes the Zomato URL, Restaurant Address, and the Name of restaurant. The file *Data extractor for each restaurant* contains the code to extract details of each of the restaurant.
"""

# Lets import all the libraries
import pandas as pd
from selenium import webdriver
chrome_path = r"C:\Users\Shweta Singh\Desktop\chromedriver.exe"

"""# Data scraping for each neighborhood

The data for each of the neighborhood listed on the zomato website are scraped separately. Just enter the url of the neighborhood in url
"""

# Url for each neighborhood
url = "https://www.zomato.com/indore/vijay-nagar-restaurants" 
# neighborhood
city = "vijaynagar"

"""#### Initialise the web driver"""

wd = webdriver.Chrome(chrome_path)
wd.get(url)

"""#### Types of restaurants in that city"""

rest_types = wd.find_element_by_xpath('//div[@class="ui vertical pointing menu subzone_category_container"]')
rest_types = rest_types.text.split("\n")
print(rest_types)
wd.quit()

# This function returns all the buttons for navigating each category
def rest_types_buttons():
    buttons = wd.find_elements_by_xpath('//span[@class="zred"]')
    return buttons

# This function returns the link, name and address of restaurant for each page
def name_link_add():
    rest_link = []
    rest_name = []
    restaurants = wd.find_elements_by_xpath('//a[@class="result-title hover_feedback zred bold ln24   fontsize0 "]')
    for name in restaurants:
        rest_link.append(name.get_attribute('href'))
        rest_name.append(name.text)
    restaurants_address =  wd.find_elements_by_xpath('//div[@class="col-m-16 search-result-address grey-text nowrap ln22"]')
    rest_address = []
    for rest_add in restaurants_address:
        rest_address.append(rest_add.text)
    return rest_link, rest_name, rest_address

# this function returns the all the data from an individual category (all pages combined)
def get_data_rest_type(rest_type):
    try:
        prev_link,prev_name,prev_add = None, None, None
        rest_link, rest_name, rest_address = name_link_add()
        link = []
        name = []
        address = []
        while(prev_link != rest_link):
            prev_link,prev_name,prev_add = rest_link, rest_name, rest_address
            link = link + prev_link
            name = name + prev_name
            address = address + prev_add
            next_page_button = wd.find_element_by_xpath('//i[@class="right angle icon"]')
            next_page_button.click()
            wd.switch_to_window(wd.window_handles[0])
            rest_link, rest_name, rest_address = name_link_add()
            # Below two if conditions are for debugging
            if((len(rest_address) == len(rest_link) == len(rest_name)) == False):
                print("need to see, name link address mismatch")
                break
            if(len(rest_name) == 0):
                print("Empty found")
    except:
        print("unknown error")
    return link, name, address

"""#### There are approx 6-7 category of restaurant for each of the restaurant. Lets scrape them individually. Since it will be easier to debug.

#### 0th Category
"""

# For each of the category
type_ = ("_".join(rest_types[0].lower().split(' ')))
print(type_)

# Go inside the category by clicking on the button on the main page
wd = webdriver.Chrome(chrome_path)
wd.get(url)
wd.switch_to_window(wd.window_handles[0])
buttons = rest_types_buttons()
buttons[0].click()
wd.switch_to_window(wd.window_handles[0])

# Collect the data
data_rest_type = get_data_rest_type(type)

# Forma dataframe
add = pd.DataFrame({'link' : data_rest_type[0], 'name' : data_rest_type[1], 'address' : data_rest_type[2]} )

# save the file in csv format
filename = city + type_ + "_.csv"
add.to_csv(filename, index=False, columns = ['link', 'address', 'name'])

# Lets verify our file name
filename

"""The code remains same for all other category below only the file name changes. It was easier to separate out the code all the categories. Since it made it easier to debug

#### 1st Category
"""

type_ = ("_".join(rest_types[1].lower().split(' ')))
print(type_)

wd = webdriver.Chrome(chrome_path)
wd.get(url)
wd.switch_to_window(wd.window_handles[0])
buttons = rest_types_buttons()
buttons[1].click()
wd.switch_to_window(wd.window_handles[0])

data_rest_type = get_data_rest_type(type_)

add = pd.DataFrame({'link' : data_rest_type[0], 'name' : data_rest_type[1], 'address' : data_rest_type[2]} )

filename = city + type_ + "_.csv"
add.to_csv(filename, index=False, columns = ['link', 'address', 'name'])

filename

"""#### 2nd Category"""

type_ = ("_".join(rest_types[2].lower().split(' ')))
print(type_)

wd = webdriver.Chrome(chrome_path)
wd.get(url)
wd.switch_to_window(wd.window_handles[0])
buttons = rest_types_buttons()
buttons[2].click()
wd.switch_to_window(wd.window_handles[0])

data_rest_type = get_data_rest_type(type_)

add = pd.DataFrame({'link' : data_rest_type[0], 'name' : data_rest_type[1], 'address' : data_rest_type[2]} )

filename = city + type_ + "_.csv"
add.to_csv(filename, index=False, columns = ['link', 'address', 'name'])

filename

"""#### 3rd Category"""

type_ = ("_".join(rest_types[3].lower().split(' ')))
print(type_)

wd = webdriver.Chrome(chrome_path)
wd.get(url)
wd.switch_to_window(wd.window_handles[0])
buttons = rest_types_buttons()
buttons[3].click()
wd.switch_to_window(wd.window_handles[0])

data_rest_type = get_data_rest_type(type_)

add = pd.DataFrame({'link' : data_rest_type[0], 'name' : data_rest_type[1], 'address' : data_rest_type[2]} )

filename = city + type_ + "_.csv"
add.to_csv(filename, index=False, columns = ['link', 'address', 'name'])

filename

"""#### 4th Category"""

type_ = ("_".join(rest_types[4].lower().split(' ')))
print(type_)

wd = webdriver.Chrome(chrome_path)
wd.get(url)
wd.switch_to_window(wd.window_handles[0])
buttons = rest_types_buttons()
buttons[4].click()
wd.switch_to_window(wd.window_handles[0])

data_rest_type = get_data_rest_type(type_)

add = pd.DataFrame({'link' : data_rest_type[0], 'name' : data_rest_type[1], 'address' : data_rest_type[2]} )

filename = city + type_ + "_.csv"
add.to_csv(filename, index=False, columns = ['link', 'address', 'name'])

filename

"""#### 5th Category"""

type_ = ("_".join(rest_types[5].lower().split(' ')))
print(type_)

wd = webdriver.Chrome(chrome_path)
wd.get(url)
wd.switch_to_window(wd.window_handles[0])
buttons = rest_types_buttons()
buttons[5].click()
wd.switch_to_window(wd.window_handles[0])

data_rest_type = get_data_rest_type(type_)

add = pd.DataFrame({'link' : data_rest_type[0], 'name' : data_rest_type[1], 'address' : data_rest_type[2]} )

filename = city + type_ + "_.csv"
add.to_csv(filename, index=False, columns = ['link', 'address', 'name'])

filename

"""#### 6th Category"""

type_ = ("_".join(rest_types[6].lower().split(' ')))
print(type_)

wd = webdriver.Chrome(chrome_path)
wd.get(url)
wd.switch_to_window(wd.window_handles[0])
buttons = rest_types_buttons()
buttons[6].click()
wd.switch_to_window(wd.window_handles[0])

data_rest_type = get_data_rest_type(type_)

add = pd.DataFrame({'link' : data_rest_type[0], 'name' : data_rest_type[1], 'address' : data_rest_type[2]} )

filename = city + type_ + "_.csv"
add.to_csv(filename, index=False, columns = ['link', 'address', 'name'])

filename
